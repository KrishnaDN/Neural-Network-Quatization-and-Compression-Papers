# Deep Neural network model compression papers
## This repo will give all the papers for quatization and compressions methods for neural networks and short video explanation of the each paper

### Before 2013</h3>
   ##### 1. Weight quantization in boltzmann machines.[\[Paper\]](https://www.sciencedirect.com/science/article/pii/089360809190077I)
   ##### 2. Multilayer feedforward neural networks with single powers-of-two weights.[\[Paper\]](https://ieeexplore.ieee.org/document/229903/)
   ##### 3. Hardware Accelerated Convolutional Neural Networks for Synthetic Vision Systems[\[Paper\]](http://yann.lecun.com/exdb/publis/pdf/farabet-iscas-10.pdf)
   ##### 4. A vlsi architecture for high performance, low-cost, on-chip learning.[\[Paper\]](https://ieeexplore.ieee.org/document/5726581/)
   ##### 5. An artificial neural network accelerator using general purpose 24 bit floating point digital signal processors.[\[Paper\]](https://ieeexplore.ieee.org/document/118695/)
   ##### 6. Probabilistic rounding in neural network learning with limited precision.[\[Paper\]](https://www.sciencedirect.com/science/article/pii/092523129290014G)
   ##### 7. Finite precision error analysis of neural network hardware implementations.[\[Paper\]](https://ieeexplore.ieee.org/document/210171/)

### 2014
   ##### 1. Compressing deep convolutional networks using vector quantization.[\[Paper\]](https://arxiv.org/pdf/1412.6115.pdf)
   ##### 2. Fixed-point feedforward deep neural network design using weights +1, 0, and- 1.[\[Paper\]](https://ieeexplore.ieee.org/document/6986082/)
   ##### 3. Exploiting linear structure within convolutional networks for efficient evaluation.[\[Paper\]](https://arxiv.org/pdf/1404.0736.pdf)
   ##### 4. Learning both weights and connections for efficient neural networks.[\[Paper\]](https://arxiv.org/pdf/1506.02626.pdf)
   ##### 5. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns.[\[Paper\]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/IS140694.pdf)

### 2015
   ##### 1. Multilayer feedforward neural networks with single powers-of-two weights.[\[Paper\]](https://ieeexplore.ieee.org/document/229903/)
   ##### 2. Binaryconnect: Training deep neural networks with binary weights during propagations.[\[Paper\]](https://arxiv.org/pdf/1511.00363.pdf)
   ##### 3. Rounding methods for neural networks with low resolution synaptic weights.[\[Paper\]](https://arxiv.org/pdf/1504.05767.pdf)
   ##### 4. Deep learning with limited numerical precision.[\[Paper\]](https://arxiv.org/pdf/1502.02551)
   ##### 5. Low precision arithmetic for deep learning.[\[Paper\]](https://www.researchgate.net/publication/269932963_Low_precision_arithmetic_for_deep_learning)
   ##### 6. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.[\[Paper\]](https://arxiv.org/pdf/1510.00149)
   ##### 7. Training binary multilayer neural networks for image classification using expectation back propgation.[\[Paper\]](https://arxiv.org/pdf/1503.03562.pdf)
   ##### 8. Fixed point optimization of deep convolutional neural networks for object recognition.[\[Paper\]](https://ieeexplore.ieee.org/document/7178146/)
   ##### 9. Compressing neural networks with the hashing trick.[\[Paper\]](https://arxiv.org/pdf/1504.04788)
   ##### 10. Convolutional neural networks at constrained time cost.[\[Paper\]](https://arxiv.org/pdf/1412.1710)
   ##### 11. Neural networks with few multiplication.[\[Paper\]](https://arxiv.org/pdf/1510.03009)
   ##### 12. Resiliency of deep neural networks under quantization.[\[Paper\]](https://arxiv.org/pdf/1511.06488.pdf)
   ##### 13. Learning both weights and connections for efficient neural network.[\[Paper\]](https://arxiv.org/pdf/1506.02626)

### 2016
   ##### 1. Qsgd: Randomized quantization for communication-optimal stochastic gradient descent.[\[Paper\]](https://arxiv.org/pdf/1610.02132)
   ##### 2. Communication quantization for data-parallel training of deep neural networks.[\[Paper\]](https://e-reports-ext.llnl.gov/pdf/833504.pdf)
   ##### 3. Effective quantization methods for recurrent neural networks.[\[Paper\]](https://arxiv.org/pdf/1611.10176)
   ##### 4. Loss-aware binarization of deep networks.[\[Paper\]](https://arxiv.org/pdf/1611.01600)
   ##### 5. Binarized neural networks.[\[Paper\]](https://arxiv.org/pdf/1602.02830)
   ##### 6. Bitwise neural networks.[\[Paper\]](https://arxiv.org/pdf/1601.06071.pdf)
   ##### 7. Ternary weight networks.[\[Paper\]](https://arxiv.org/pdf/1605.04711)
   ##### 8. Overcoming challenges in fixed point training of deep convolutional networks.[\[Paper\]](https://arxiv.org/pdf/1607.02241)
   ##### 9. Fixed point quantization of deep convolutional networks.[\[Paper\]](https://arxiv.org/pdf/1511.06393)
   ##### 10. Deep neural networks are robust to weight binarization and other non-linear distortions.[\[Paper\]](https://arxiv.org/pdf/1606.01981)
   ##### 11. Sigma delta quantized networks.[\[Paper\]](https://arxiv.org/pdf/1611.02024)
   ##### 12. Recurrent neural networks with limited numerical precision.[\[Paper\]](https://arxiv.org/pdf/1608.06902)
   ##### 13. Xnor-net: Imagenet classification using binary convolutional neural networks.[\[Paper\]](https://arxiv.org/pdf/1603.05279.pdf)
   ##### 14. Training bit fully convolutional network for fast semantic segmentation.[\[Paper\]](https://arxiv.org/pdf/1612.00212.pdf)
   ##### 15. Quantized convolutional neural networks for mobile devices.[\[Paper\]](https://ieeexplore.ieee.org/document/7780890/)
   ##### 16. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients.[\[Paper\]](https://arxiv.org/pdf/1606.06160.pdf)
   ##### 17. Trained ternary quantization.[\[Paper\]](https://arxiv.org/pdf/1612.01064.pdf)

### 2017
   ##### 1. The high-dimensional geometry of binary neural networks.[\[Paper\]](https://arxiv.org/pdf/1705.07199.pdf)
   ##### 2. Deep learning with low precision by half-wave gaussian quantization.[\[Paper\]](https://arxiv.org/pdf/1702.00953)
   ##### 3. Model compression as constrained optimization, with application to neural nets.[\[Paper\]](https://arxiv.org/pdf/1707.01209)
   ##### 4. A contextual discretization framework for compressing recurrent neural networks.[\[Paper\]](https://unify.id/wp-content/uploads/2018/03/RNN_compress.pdf)
   ##### 5. Gated xnor networks: Deep neural networks with ternary weights and activations under a unified discretization. framework.[\[Paper\]](https://arxiv.org/pdf/1705.09283.pdf)
   ##### 6. Shiftcnn: Generalized low-precision architecture for inference of convolutional neural networks.[\[Paper\]](https://arxiv.org/pdf/1706.02393)
   ##### 7. Network sketching: Exploiting binary structure in deep cnns.[\[Paper\]](https://arxiv.org/pdf/1706.02021)
   ##### 8. Training quantized nets: A deeper understanding.[\[Paper\]](https://arxiv.org/pdf/1706.02379)
   ##### 9. Towards accurate binary convolutional neural network.[\[Paper\]](https://arxiv.org/pdf/1711.11294)
   ##### 10. Ternary neural networks with finegrained quantization.[\[Paper\]](https://arxiv.org/pdf/1705.01462)
   ##### 11. Wrpn: Wide reduced-precision networks.[\[Paper\]](https://arxiv.org/pdf/1709.01134)
   ##### 12. Minimum energy quantized neural networks.[\[Paper\]](https://arxiv.org/pdf/1711.00215)
   ##### 13. Weighted-entropy-based quantization for deep neural networks.[\[Paper\]](https://ieeexplore.ieee.org/document/8100244/)
   ##### 14. Learning discrete weights using the local reparameterization trick.[\[Paper\]](https://arxiv.org/pdf/1710.07739.pdf)
   ##### 15. How to train a compact binary neural network with high accuracy?.[\[Paper\]](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14619/14454)
   ##### 16. Fixed-point factorized networks.[\[Paper\]](https://arxiv.org/pdf/1611.01972)
   ##### 17. Terngrad: Ternary gradients to reduce communication in distributed deep learning.[\[Paper\]](https://arxiv.org/pdf/1705.07878.pdf)
   ##### 18. Incremental network quantization: Towards cnns with low-precision weights.[\[Paper\]](https://arxiv.org/pdf/1702.03044.pdf)
   ##### 19. Adaptive quantization for deep neural network.[\[Paper\]](https://arxiv.org/pdf/1712.01048.pdf)

### 2018
   ##### 1. Alternating multi-bit quantization for recurrent neural networks.[\[Paper\]](https://arxiv.org/pdf/1802.00150)
   ##### 2. Variational network quantization.[\[Paper\]](https://pdfs.semanticscholar.org/f93a/e1a0b9e40b138da8c25855b6c68af4ee201a.pdf)
   ##### 3. Loss-aware weight quantization of deep networks.[\[Paper\]](https://arxiv.org/pdf/1802.08635)
   ##### 4. Bit-regularized optimization of neural nets.[\[Paper\]](https://arxiv.org/pdf/1708.04788)
   ##### 5. Model compression via distillation and quantization.[\[Paper\]](https://arxiv.org/pdf/1802.05668)
   ##### 6. Training and inference with integers in deep neural networks.[\[Paper\]](https://arxiv.org/pdf/1802.04680.pdf)
   ##### 7. Adaptive quantization of neural networks.[\[Paper\]](https://openreview.net/pdf?id=SyOK1Sg0W)

#### A Huge Thanks to Yunhui Guo(https://yunhuiguo.github.io/) for his paper on survey of neural network quantization. Check out his paper https://arxiv.org/pdf/1808.04752.pdf.
