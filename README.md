# Deep Neural network model compression papers
## This repo will give all the papers for quatization and compressions methods for neural networks and short video explanation of the each paper
<span style="color: green"> Some green text </span>
### Before 2013</h3>
   ##### 1. Weight quantization in boltzmann machines.[\[Paper\]](https://www.sciencedirect.com/science/article/pii/089360809190077I)
   ##### 2. Multilayer feedforward neural networks with single powers-of-two weights.[\[Paper\]]()
   ##### 3. Hardware Accelerated Convolutional Neural Networks for Synthetic Vision Systems A vlsi architecture for high performance, low-cost, on-chip learning.[\[Paper\]]()
   ##### 4. An artificial neural network accelerator using general purpose 24 bit floating point digital signal processors.[\[Paper\]]()
   ##### 5. Probabilistic rounding in neural network learning with limited precision.[\[Paper\]]()
   ##### 6. Finite precision error analysis of neural network hardware implementations.[\[Paper\]]()

### 2014
   ##### 1. Compressing deep convolutional networks using vector quantization.[\[Paper\]]()
   ##### 2. Fixed-point feedforward deep neural network design using weights +1, 0, and- 1.[\[Paper\]]()
   ##### 3. Exploiting linear structure within convolutional networks for efficient evaluation.[\[Paper\]]()
   ##### 4. Learning both weights and connections for efficient neural networks.[\[Paper\]]()
   ##### 5. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns.[\[Paper\]]()

### 2015
   ##### 1. Multilayer feedforward neural networks with single powers-of-two weights.[\[Paper\]]()
   ##### 2. Binaryconnect: Training deep neural networks with binary weights during propagations.[\[Paper\]]()
   ##### 3. Rounding methods for neural networks with low resolution synaptic weights.[\[Paper\]]()
   ##### 4. Deep learning with limited numerical precision.[\[Paper\]]()
   ##### 5. Low precision arithmetic for deep learning.[\[Paper\]]()
   ##### 6. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.[\[Paper\]]()
   ##### 7. Training binary multilayer neural networks for image classification using expectation back propgation.[\[Paper\]]()
   ##### 8. Fixed point optimization of deep convolutional neural networks for object recognition.[\[Paper\]]()
   ##### 9. Compressing neural networks with the hashing trick.[\[Paper\]]()
   ##### 10. Convolutional neural networks at constrained time cost.[\[Paper\]]()
   ##### 11. Neural networks with few multiplication.[\[Paper\]]()
   ##### 12. Resiliency of deep neural networks under quantization.[\[Paper\]]()
   ##### 13. Learning both weights and connections for efficient neural network.[\[Paper\]]()

### 2016
   ##### 1. Qsgd: Randomized quantization for communication-optimal stochastic gradient descent.[\[Paper\]]()
   ##### 2. Communication quantization for data-parallel training of deep neural networks.[\[Paper\]]()
   ##### 3. Effective quantization methods for recurrent neural networks.[\[Paper\]]()
   ##### 4. Loss-aware binarization of deep networks.[\[Paper\]]()
   ##### 5. Binarized neural networks.[\[Paper\]]()
   ##### 6. Bitwise neural networks.[\[Paper\]]()
   ##### 7. Ternary weight networks.[\[Paper\]]()
   ##### 8. Overcoming challenges in fixed point training of deep convolutional networks.[\[Paper\]]()
   ##### 9. Fixed point quantization of deep convolutional networks.[\[Paper\]]()
   ##### 10. Deep neural networks are robust to weight binarization and other non-linear distortions.[\[Paper\]]()
   ##### 11. Sigma delta quantized networks.[\[Paper\]]()
   ##### 12. Recurrent neural networks with limited numerical precision.[\[Paper\]]()
   ##### 13. Xnor-net: Imagenet classification using binary convolutional neural networks.[\[Paper\]]()
   ##### 14. Training bit fully convolutional network for fast semantic segmentation.[\[Paper\]]()
   ##### 15. Quantized convolutional neural networks for mobile devices.[\[Paper\]]()
   ##### 16. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients.[\[Paper\]]()
   ##### 17. Trained ternary quantization.[\[Paper\]]()

### 2017
   ##### 1. The high-dimensional geometry of binary neural networks.[\[Paper\]]()
   ##### 2. Deep learning with low precision by half-wave gaussian quantization.[\[Paper\]]()
   ##### 3. Model compression as Â´ constrained optimization, with application to neural nets.[\[Paper\]]()
   ##### 4. A contextual discretization framework for compressing recurrent neural networks.[\[Paper\]]()
   ##### 5. Gated xnor networks: Deep neural networks with ternary weights and activations under a unified discretization. framework.[\[Paper\]]()
   ##### 6. Shiftcnn: Generalized low-precision architecture for inference of convolutional neural networks.[\[Paper\]]()
   ##### 7. Network sketching: Exploiting binary structure in deep cnns.[\[Paper\]]()
   ##### 8. Training quantized nets: A deeper understanding.[\[Paper\]]()
   ##### 9. Towards accurate binary convolutional neural network.[\[Paper\]]()
   ##### 10. Ternary neural networks with finegrained quantization.[\[Paper\]]()
   ##### 11. Wrpn: Wide reduced-precision networks.[\[Paper\]]()
   ##### 12. Minimum energy quantized neural networks.[\[Paper\]]()
   ##### 13. Weighted-entropy-based quantization for deep neural networks.[\[Paper\]]()
   ##### 14. Learning discrete weights using the local reparameterization trick.[\[Paper\]]()
   ##### 15. How to train a compact binary neural network with high accuracy?.[\[Paper\]]()
   ##### 16. Fixed-point factorized networks.[\[Paper\]]()
   ##### 17. Terngrad: Ternary gradients to reduce communication in distributed deep learning.[\[Paper\]]()
   ##### 18. Incremental network quantization: Towards cnns with low-precision weights.[\[Paper\]]()
   ##### 19. Adaptive quantization for deep neural network.[\[Paper\]]()

### 2018
   ##### 1. Alternating multi-bit quantization for recurrent neural networks.[\[Paper\]]()
   ##### 2. Variational network quantization.[\[Paper\]]()
   ##### 3. Loss-aware weight quantization of deep networks.[\[Paper\]]()
   ##### 4. Bit-regularized optimization of neural nets.[\[Paper\]]()
   ##### 5. Model compression via distillation and quantization.[\[Paper\]]()
   ##### 6. Training and inference with integers in deep neural networks.[\[Paper\]]()
   ##### 7. Adaptive quantization of neural networks.[\[Paper\]]()

### A Huge Thanks to Yunhui Guo(https://yunhuiguo.github.io/) for his paper on survey of neural network quantization. Check out his paper https://arxiv.org/pdf/1808.04752.pdf. some of the content is taken from this paper
