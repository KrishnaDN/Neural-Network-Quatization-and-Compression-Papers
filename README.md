# Deep Neural network model compression papers
## This repo will give all the papers for quatization and compressions methods for neural networks and short video explanation of the each paper

### Before 2013</h3>
   ##### 1. Weight quantization in boltzmann machines.
   ##### 2. Multilayer feedforward neural networks with single powers-of-two weights.
   ##### 3. Hardware Accelerated Convolutional Neural Networks for Synthetic Vision Systems A vlsi architecture for high performance, low-cost, on-chip learning
   ##### 4. An artificial neural network accelerator using general purpose 24 bit floating point digital signal processors
   ##### 5. Probabilistic rounding in neural network learning with limited precision
   ##### 6. Finite precision error analysis of neural network hardware implementations

### 2014
   ##### 1. Compressing deep convolutional networks using vector quantization
   ##### 2. Fixed-point feedforward deep neural network design using weights +1, 0, and- 1
   ##### 3. Exploiting linear structure within convolutional networks for efficient evaluation
   ##### 4. Learning both weights and connections for efficient neural networks
   ##### 5. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns

### 2015
   ##### 1. Multilayer feedforward neural networks with single powers-of-two weights
   ##### 2. Binaryconnect: Training deep neural networks with binary weights during propagations
   ##### 3. Rounding methods for neural networks with low resolution synaptic weights
   ##### 4. Deep learning with limited numerical precision
   ##### 5. Low precision arithmetic for deep learning
   ##### 6. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding
   ##### 7. Training binary multilayer neural networks for image classification using expectation back propgation
   ##### 8. Fixed point optimization of deep convolutional neural networks for object recognition.
   ##### 9. Compressing neural networks with the hashing trick
   ##### 10. Convolutional neural networks at constrained time cost
   ##### 11. Neural networks with few multiplication
   ##### 12. Resiliency of deep neural networks under quantization
   ##### 13. Learning both weights and connections for efficient neural network.

### 2016
   ##### 1. Qsgd: Randomized quantization for communication-optimal stochastic gradient descent
   ##### 2. Communication quantization for data-parallel training of deep neural networks
   ##### 3. Effective quantization methods for recurrent neural networks
   ##### 4. Loss-aware binarization of deep networks
   ##### 5. Binarized neural networks
   ##### 6. Bitwise neural networks
   ##### 7. Ternary weight networks
   ##### 8. Overcoming challenges in fixed point training of deep convolutional networks
   ##### 9. Fixed point quantization of deep convolutional networks
   ##### 10. Deep neural networks are robust to weight binarization and other non-linear distortions
   ##### 11. Sigma delta quantized networks
   ##### 12. Recurrent neural networks with limited numerical precision
   ##### 13. Xnor-net: Imagenet classification using binary convolutional neural networks
   ##### 14. Training bit fully convolutional network for fast semantic segmentation
   ##### 15. Quantized convolutional neural networks for mobile devices
   ##### 16. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients
   ##### 17. Trained ternary quantization

### 2017
   ##### 1. The high-dimensional geometry of binary neural networks
   ##### 2. Deep learning with low precision by half-wave gaussian quantization
   ##### 3. Model compression as Â´ constrained optimization, with application to neural nets
   ##### 4. A contextual discretization framework for compressing recurrent neural networks
   ##### 5. Gated xnor networks: Deep neural networks with ternary weights and activations under a unified discretization framework
   ##### 6. Shiftcnn: Generalized low-precision architecture for inference of convolutional neural networks
   ##### 7. Network sketching: Exploiting binary structure in deep cnns
   ##### 8. Training quantized nets: A deeper understanding
   ##### 9. Towards accurate binary convolutional neural network.
   ##### 10. Ternary neural networks with finegrained quantization
   ##### 11. Wrpn: Wide reduced-precision networks
   ##### 12. Minimum energy quantized neural networks
   ##### 13. Weighted-entropy-based quantization for deep neural networks
   ##### 14. Learning discrete weights using the local reparameterization trick
   ##### 15. How to train a compact binary neural network with high accuracy?
   ##### 16. Fixed-point factorized networks
   ##### 17. Terngrad: Ternary gradients to reduce communication in distributed deep learning
   ##### 18. Incremental network quantization: Towards cnns with low-precision weights
   ##### 19. Adaptive quantization for deep neural network

### 2018
   ##### 1. Alternating multi-bit quantization for recurrent neural networks.
   ##### 2. Variational network quantization
   ##### 3. Loss-aware weight quantization of deep networks.
   ##### 4. Bit-regularized optimization of neural nets
   ##### 5. Model compression via distillation and quantization
   ##### 6. Training and inference with integers in deep neural networks
   ##### 7. Adaptive quantization of neural networks

### A Huge Thanks to Yunhui Guo(https://yunhuiguo.github.io/) for his paper on survey of neural network quantization. Check out his paper https://arxiv.org/pdf/1808.04752.pdf. some of the content is taken from this paper
