# Deep Neural network model compression papers
<h2>This repo will give all the papers for quatization and compressions methods for neural networks and short video explanation of the each paper.</h2>

<span style="color: green"> Some green text </span>
<h3>Before 2013</h3>
     <h4> 1. Weight quantization in boltzmann machines</h4>
      <h4>2. Multilayer feedforward neural networks with single powers-of-two weights.</h4>
      <h4>3. Hardware Accelerated Convolutional Neural Networks for Synthetic Vision Systems A vlsi architecture for high    performance, low-cost, on-chip learning</h4>
      <h4>4. An artificial neural network accelerator using general purpose 24 bit floating point digital signal processors</h4>
      <h4>5. Probabilistic rounding in neural network learning with limited precision</h4>
      <h4>6. Finite precision error analysis of neural network hardware implementations</h4>

<h3>2014</h3>
  <h4>1. Compressing deep convolutional networks using vector quantization</h4>
  <h4>2. Fixed-point feedforward deep neural network design using weights +1, 0, and- 1</h4>
  <h4>3. Exploiting linear structure within convolutional networks for efficient evaluation</h4>
  <h4>4. Learning both weights and connections for efficient neural networks</h4>
  <h4>5. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns</h4>

<h3>2015</h3>
  <h4>1. Multilayer feedforward neural networks with single powers-of-two weights</h4>
  <h4>2. Binaryconnect: Training deep neural networks with binary weights during propagations</h4>
  <h4>3. Rounding methods for neural networks with low resolution synaptic weights</h4>
  <h4>4. Deep learning with limited numerical precision</h4>
  <h4>5. Low precision arithmetic for deep learning</h4>
  <h4>6. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</h4>
  <h4>7. Training binary multilayer neural networks for image classification using expectation back propgation</h4>
  <h4>8. Fixed point optimization of deep convolutional neural networks for object recognition.</h4>
  <h4>9. Compressing neural networks with the hashing trick</h4>
  <h4>10. Convolutional neural networks at constrained time cost</h4>
  <h4>11. Neural networks with few multiplication</h4>
  <h4>12. Resiliency of deep neural networks under quantization</h4>
  <h4>13. Learning both weights and connections for efficient neural network.</h4>

<h3>2016</h3>
  <h4>1. Qsgd: Randomized quantization for communication-optimal stochastic gradient descent</h4>
  <h4>2. Communication quantization for data-parallel training of deep neural networks</h4>
  <h4>3. Effective quantization methods for recurrent neural networks</h4>
  <h4>4. Loss-aware binarization of deep networks</h4>
  <h4>5. Binarized neural networks</h4>
  <h4>6. Bitwise neural networks</h4>
  <h4>7. Ternary weight networks</h4>
  <h4>8. Overcoming challenges in fixed point training of deep convolutional networks</h4>
  <h4>9. Fixed point quantization of deep convolutional networks</h4>
  <h4>10. Deep neural networks are robust to weight binarization and other non-linear distortions</h4>
  <h4>11. Sigma delta quantized networks</h4>
  <h4>12. Recurrent neural networks with limited numerical precision</h4>
  <h4>13. Xnor-net: Imagenet classification using binary convolutional neural networks</h4>
  <h4>14. Training bit fully convolutional network for fast semantic segmentation</h4>
  <h4>15. Quantized convolutional neural networks for mobile devices</h4>
  <h4>16. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients</h4>
  <h4>17. Trained ternary quantization</h4>

  <h3>2017</h3>
  <h4>1. The high-dimensional geometry of binary neural networks</h4>
  <h4>2. Deep learning with low precision by half-wave gaussian quantization</h4>
  <h4>3. Model compression as Â´ constrained optimization, with application to neural nets</h4>
  <h4>4. A contextual discretization framework for compressing recurrent neural networks</h4>
  <h4>5. Gated xnor networks: Deep neural networks with ternary weights and activations under a unified discretization framework</h4>
  <h4>6. Shiftcnn: Generalized low-precision architecture for inference of convolutional neural networks</h4>
  <h4>7. Network sketching: Exploiting binary structure in deep cnns</h4>
  <h4>8. Training quantized nets: A deeper understanding</h4>
  <h4>9. Towards accurate binary convolutional neural network.</h4>
  <h4>10. Ternary neural networks with finegrained quantization</h4>
  <h4>11. Wrpn: Wide reduced-precision networks</h4>
  <h4>12. Minimum energy quantized neural networks</h4>
  <h4>13. Weighted-entropy-based quantization for deep neural networks</h4>
  <h4>14. Learning discrete weights using the local reparameterization trick</h4>
  <h4>15. How to train a compact binary neural network with high accuracy?</h4>
  <h4>16. Fixed-point factorized networks</h4>
  <h4>17. Terngrad: Ternary gradients to reduce communication in distributed deep learning</h4>
  <h4>18. Incremental network quantization: Towards cnns with low-precision weights</h4>
  <h4>19. Adaptive quantization for deep neural network</h4>

<h3>2018</h3>
  <h4>1. Alternating multi-bit quantization for recurrent neural networks.</h4>
  <h4>2. Variational network quantization</h4>
  <h4>3. Loss-aware weight quantization of deep networks.</h4>
  <h4>4. Bit-regularized optimization of neural nets</h4>
  <h4>5. Model compression via distillation and quantization</h4>
  <h4>6. Training and inference with integers in deep neural networks</h4>
  <h4>7. Adaptive quantization of neural networks</h4>

<h3>A Huge Thanks to Yunhui Guo(https://yunhuiguo.github.io/) for his paper on survey of neural network quantization. Check out his paper https://arxiv.org/pdf/1808.04752.pdf. some of the content is taken from this paper.</h3>
